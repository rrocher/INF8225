{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os.path\n",
    "from argparse import ArgumentParser\n",
    "from tempfile import mkstemp\n",
    "from io import StringIO, BytesIO\n",
    "from lxml import etree\n",
    "import os\n",
    "import re\n",
    "import stat\n",
    "import fileinput\n",
    "import subprocess\n",
    "import sys, traceback,logging\n",
    "import json\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class LogLine(object):\n",
    "\tdef __init__(self, msg_type=None, line_id = None, params = None):\n",
    "\t\tself.msg_type = msg_type\n",
    "\t\tself.line_id = line_id\n",
    "\t\tself.params = params\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\tif not self.params:\n",
    "\t\t\tmsg2 = ''\n",
    "\t\telse:\n",
    "\t\t\tll = [str(x) for x in self.params]\n",
    "\t\t\tmsg2 = ' '.join(ll)\n",
    "\t\treturn '('+str(self.line_id)+'-args['+msg2+'])'\n",
    "\n",
    "class LCSObject(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.lcs_seq = []\n",
    "\t\tself.log_lines = []\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\tmsg1 = ' '.join(self.lcs_seq)\n",
    "\t\tll = [str(x) for x in self.log_lines]\n",
    "\t\tmsg2 = ','.join(ll)\n",
    "\t\treturn '{\"'+msg1+'\"}\\n'\n",
    "\n",
    "def lcs(a,b):\n",
    "\tlengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "\tfor i, x in enumerate(a):\n",
    "\t\tfor j, y in enumerate(b):\n",
    "\t\t\tif x == y:\n",
    "\t\t\t\tlengths[i+1][j+1] = lengths[i][j] + 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tlengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "\tresult = []\n",
    "\trlen = 0\n",
    "\tx,y = len(a),len(b)\n",
    "\tparams_for_a = []\n",
    "\tparams_for_b = []\n",
    "\tresx = True\n",
    "\tif x < y:\n",
    "\t\tresx = False\n",
    "\twhile x != 0 and y != 0:\n",
    "\t\tif lengths[x][y] == lengths[x-1][y]:\n",
    "\t\t\tx -= 1\n",
    "\t\t\tif resx:\n",
    "\t\t\t\tresult.insert(0,'*')\n",
    "\t\t\tparams_for_a.insert(0 , a[x])\n",
    "\t\t\tif lengths[x][y] == 0:\n",
    "\t\t\t\tparams_for_b.insert(0, b[y-1])\n",
    "\t\telif lengths[x][y] == lengths[x][y-1]:\n",
    "\t\t\ty -= 1\n",
    "\t\t\tif resx is False:\n",
    "\t\t\t\tresult.insert(0,'*')\n",
    "\t\t\tparams_for_b.insert(0,b[y])\n",
    "\t\t\tif lengths[x][y] == 0:\n",
    "\t\t\t\tparams-for_a.insert(0, b[x-1])\n",
    "\t\telse:\n",
    "\t\t\tassert a[x-1] == b[y-1]\n",
    "\t\t\tresult.insert(0, a[x-1])\n",
    "\t\t\trlen = rlen + 1\n",
    "\t\t\tx -= 1\n",
    "\t\t\ty -= 1\n",
    "\treturn rlen, result, params_for_a, params_for_b\n",
    "\t\t\n",
    "\n",
    "def checkParams(paramsL):\n",
    "\tfor param in paramsL:\n",
    "\t\tif param == '*':\n",
    "\t\t\treturn False\n",
    "\treturn True\n",
    "\n",
    "def search(lmap, seq, seqLine):\n",
    "\tmaxLen = 0\n",
    "\tmaxLiPos = -1\n",
    "\tlcsObjFound = None\n",
    "\tfinalLcs = None\n",
    "\tfinalParamsA = None\n",
    "\tfinalParamsB = None\n",
    "\tif len(lmap) == 0:\n",
    "\t\treturn None\n",
    "\n",
    "\tfor pos, lcsObj in enumerate(lmap):\n",
    "\t\ttmpLen, tmp, params_for_a, params_for_b = lcs(lcsObj.lcs_seq, seq)\n",
    "\t\tif tmpLen > maxLen:\n",
    "\t\t\tmaxLen = tmpLen\n",
    "\t\t\tmaxLiPos= pos\n",
    "\t\t\tlcsObjFound = lcsObj\n",
    "\t\t\tfinalLcs = tmp\n",
    "\t\t\tfinalParamsA = params_for_a\n",
    "\t\t\tfinalParamsB = params_for_b\n",
    "\tif maxLen > (len(seq)/2):\n",
    "\t\tlmap[maxLiPos].lcs_seq = finalLcs\n",
    "\t\tif checkParams(finalParamsA):\n",
    "\t\t\tfor n, t in enumerate(lmap[maxLiPos].log_lines):\n",
    "\t\t\t\tif t.params is None or len(t.params) == 0:\n",
    "\t\t\t\t\tt.params = finalParamsA\n",
    "\t\tlmap[maxLiPos].log_lines.append(LogLine(finalLcs, seqLine+1, finalParamsB))\n",
    "\t\treturn lcsObjFound\n",
    "\treturn None\n",
    "\n",
    "def processMsg(msg, lmap, pos):\n",
    "\tseq = msg.split()\n",
    "\trc = search(lmap, seq, pos)\n",
    "\tif rc is not None:\n",
    "\t\treturn\n",
    "\tlcsObj = LCSObject()\n",
    "\tlcsObj.lcs_seq = seq\n",
    "\tlcsObj.log_lines.append(LogLine(seq, pos+1))\n",
    "\tlmap.append(lcsObj)\n",
    "\n",
    "\n",
    "def readFile(filename):\n",
    "\twith open(filename, 'r', encoding = 'utf-8') as f:\n",
    "\t\tcontent = f.readlines()\n",
    "\treturn content\n",
    "\n",
    "def runXmlLcs(lmap, content, msgattrib):\n",
    "    for pos, line in enumerate(content):\n",
    "        root = etree.fromstring(line)\n",
    "        msg = root.attrib[msgattrib]\n",
    "        processMsg(msg, lmap, pos)\n",
    "    return lmap\n",
    "\n",
    "def runTextLogLcs(lmap, content, offset):\n",
    "    for pos, line in enumerate(content):\n",
    "        ll = line.split()\n",
    "        msg = ' '.join(ll[offset:]) \n",
    "        processMsg(msg, lmap, pos)\n",
    "    return lmap\n",
    "\n",
    "\n",
    "def main(filename, logtype, offset=None, msgattrib=None):\n",
    "    lcsmap = []\n",
    "    filecontent = readFile(filename)\n",
    "    if logtype == 'text':\n",
    "        runTextLogLcs(lcsmap, filecontent, offset)\n",
    "    elif logtype == 'xml':\n",
    "        runXmlLcs(lcsmap, filecontent, msgattrib)\n",
    "    return lcsmap\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(dictionary, reverse_dictionary):\n",
    "    vocab_size = len(dictionary)\n",
    "\n",
    "    #tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "    y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "    # RNN output node weights and biases\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "    }\n",
    "    pred = RNN(x, weights, biases)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Model evaluation\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Launch the graph\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        step = 0\n",
    "        offset = random.randint(0,n_input+1)\n",
    "        end_offset = n_input + 1\n",
    "        acc_total = 0\n",
    "        loss_total = 0\n",
    "\n",
    "        writer.add_graph(session.graph)\n",
    "\n",
    "        while step < training_iters:\n",
    "            # Generate a minibatch. Add some randomness on selection process.\n",
    "            if offset > (len(training_data)-end_offset):\n",
    "                offset = random.randint(0, n_input+1)\n",
    "\n",
    "            symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "            symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "            symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "            symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "            symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "            _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                    feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "            loss_total += loss\n",
    "            acc_total += acc\n",
    "            if (step+1) % display_step == 0:\n",
    "                print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                      \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "                acc_total = 0\n",
    "                loss_total = 0\n",
    "                symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "                symbols_out = training_data[offset + n_input]\n",
    "                symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "                print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "            step += 1\n",
    "            offset += (n_input+1)\n",
    "        print(\"Optimization Finished!\")\n",
    "        print(\"Run on command line.\")\n",
    "        print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "        print(\"Point your web browser to: http://localhost:6006/\")\n",
    "        return pred\n",
    "    \n",
    "def test(pred, dictionary, reverse_dictionary):\n",
    "    with tf.Session() as session:\n",
    "        while True:\n",
    "            prompt = \"%s words: \" % n_input\n",
    "            sentence = input(prompt)\n",
    "            sentence = sentence.strip()\n",
    "            words = sentence.split(' ')\n",
    "            if len(words) != n_input:\n",
    "                continue\n",
    "            try:\n",
    "                symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "                for i in range(32):\n",
    "                    keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                    onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                    onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                    sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                    symbols_in_keys = symbols_in_keys[1:]\n",
    "                    symbols_in_keys.append(onehot_pred_index)\n",
    "                print(sentence)\n",
    "            except:\n",
    "                print(\"Word not in dictionary\")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    lcsmap = main('hadoop.log', 'text',offset=2 )\n",
    "    lcsseqs  = [ ' '.join(x.lcs_seq) for x in lcsmap]\n",
    "    dictionary, reverse_dictionary = build_dataset(lcsseqs)\n",
    "    train(dictionary, reverse_dictionary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
